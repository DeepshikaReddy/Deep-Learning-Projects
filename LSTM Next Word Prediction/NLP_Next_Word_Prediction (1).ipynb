{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79658065",
   "metadata": {},
   "source": [
    "## Next Word Prediction \n",
    "### This exercise aims at feature engineering, understanding basic LSTM models and cheking the prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11cab89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpous length 581888\n"
     ]
    }
   ],
   "source": [
    "path='1661-0.txt'\n",
    "text=open(path,encoding='utf-8').read().lower()\n",
    "print(\"corpous length\",len(text)) #Entire document is called Corpus D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0e33e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 's',\n",
       " 'the',\n",
       " 'adventures',\n",
       " 'of',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'by',\n",
       " 'arthur',\n",
       " 'conan',\n",
       " 'doyle',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'almost',\n",
       " 'no',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " 'you',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'it',\n",
       " 'give',\n",
       " 'it',\n",
       " 'away',\n",
       " 'or',\n",
       " 're',\n",
       " 'use',\n",
       " 'it',\n",
       " 'under',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'license',\n",
       " 'included',\n",
       " 'with',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'or',\n",
       " 'online',\n",
       " 'at',\n",
       " 'www',\n",
       " 'gutenberg',\n",
       " 'net',\n",
       " 'title',\n",
       " 'the',\n",
       " 'adventures',\n",
       " 'of',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'author',\n",
       " 'arthur',\n",
       " 'conan',\n",
       " 'doyle',\n",
       " 'release',\n",
       " 'date',\n",
       " 'november',\n",
       " '29',\n",
       " '2002',\n",
       " 'ebook',\n",
       " '1661',\n",
       " 'last',\n",
       " 'updated',\n",
       " 'may',\n",
       " '20',\n",
       " '2019',\n",
       " 'language',\n",
       " 'english',\n",
       " 'character',\n",
       " 'set',\n",
       " 'encoding',\n",
       " 'utf',\n",
       " '8',\n",
       " 'start',\n",
       " 'of',\n",
       " 'this',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'the',\n",
       " 'adventures',\n",
       " 'of',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'produced',\n",
       " 'by',\n",
       " 'an',\n",
       " 'anonymous',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'volunteer',\n",
       " 'and',\n",
       " 'jose',\n",
       " 'menendez',\n",
       " 'cover',\n",
       " 'the',\n",
       " 'adventures',\n",
       " 'of',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'by',\n",
       " 'arthur',\n",
       " 'conan',\n",
       " 'doyle',\n",
       " 'contents',\n",
       " 'i',\n",
       " 'a',\n",
       " 'scandal',\n",
       " 'in',\n",
       " 'bohemia',\n",
       " 'ii',\n",
       " 'the',\n",
       " 'red',\n",
       " 'headed',\n",
       " 'league',\n",
       " 'iii',\n",
       " 'a',\n",
       " 'case',\n",
       " 'of',\n",
       " 'identity',\n",
       " 'iv',\n",
       " 'the',\n",
       " 'boscombe',\n",
       " 'valley',\n",
       " 'mystery',\n",
       " 'v',\n",
       " 'the',\n",
       " 'five',\n",
       " 'orange',\n",
       " 'pips',\n",
       " 'vi',\n",
       " 'the',\n",
       " 'man',\n",
       " 'with',\n",
       " 'the',\n",
       " 'twisted',\n",
       " 'lip',\n",
       " 'vii',\n",
       " 'the',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'blue',\n",
       " 'carbuncle',\n",
       " 'viii',\n",
       " 'the',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'speckled',\n",
       " 'band',\n",
       " 'ix',\n",
       " 'the',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'engineer',\n",
       " 's',\n",
       " 'thumb',\n",
       " 'x',\n",
       " 'the',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'noble',\n",
       " 'bachelor',\n",
       " 'xi',\n",
       " 'the',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'beryl',\n",
       " 'coronet',\n",
       " 'xii',\n",
       " 'the',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'copper',\n",
       " 'beeches',\n",
       " 'i',\n",
       " 'a',\n",
       " 'scandal',\n",
       " 'in',\n",
       " 'bohemia',\n",
       " 'i',\n",
       " 'to',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'she',\n",
       " 'is',\n",
       " 'always',\n",
       " '_the_',\n",
       " 'woman',\n",
       " 'i',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention',\n",
       " 'her',\n",
       " 'under',\n",
       " 'any',\n",
       " 'other',\n",
       " 'name',\n",
       " 'in',\n",
       " 'his',\n",
       " 'eyes',\n",
       " 'she',\n",
       " 'eclipses',\n",
       " 'and',\n",
       " 'predominates',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'of',\n",
       " 'her',\n",
       " 'sex',\n",
       " 'it',\n",
       " 'was',\n",
       " 'not',\n",
       " 'that',\n",
       " 'he',\n",
       " 'felt',\n",
       " 'any',\n",
       " 'emotion',\n",
       " 'akin',\n",
       " 'to',\n",
       " 'love',\n",
       " 'for',\n",
       " 'irene',\n",
       " 'adler',\n",
       " 'all',\n",
       " 'emotions',\n",
       " 'and',\n",
       " 'that',\n",
       " 'one',\n",
       " 'particularly',\n",
       " 'were',\n",
       " 'abhorrent',\n",
       " 'to',\n",
       " 'his',\n",
       " 'cold',\n",
       " 'precise',\n",
       " 'but',\n",
       " 'admirably',\n",
       " 'balanced',\n",
       " 'mind',\n",
       " 'he',\n",
       " 'was',\n",
       " 'i',\n",
       " 'take',\n",
       " 'it',\n",
       " 'the',\n",
       " 'most',\n",
       " 'perfect',\n",
       " 'reasoning',\n",
       " 'and',\n",
       " 'observing',\n",
       " 'machine',\n",
       " 'that',\n",
       " 'the',\n",
       " 'world',\n",
       " 'has',\n",
       " 'seen',\n",
       " 'but',\n",
       " 'as',\n",
       " 'a',\n",
       " 'lover',\n",
       " 'he',\n",
       " 'would',\n",
       " 'have',\n",
       " 'placed',\n",
       " 'himself',\n",
       " 'in',\n",
       " 'a',\n",
       " 'false',\n",
       " 'position',\n",
       " 'he',\n",
       " 'never',\n",
       " 'spoke',\n",
       " 'of',\n",
       " 'the',\n",
       " 'softer',\n",
       " 'passions',\n",
       " 'save',\n",
       " 'with',\n",
       " 'a',\n",
       " 'gibe',\n",
       " 'and',\n",
       " 'a',\n",
       " 'sneer',\n",
       " 'they',\n",
       " 'were',\n",
       " 'admirable',\n",
       " 'things',\n",
       " 'for',\n",
       " 'the',\n",
       " 'observer',\n",
       " 'excellent',\n",
       " 'for',\n",
       " 'drawing',\n",
       " 'the',\n",
       " 'veil',\n",
       " 'from',\n",
       " 'men',\n",
       " 's',\n",
       " 'motives',\n",
       " 'and',\n",
       " 'actions',\n",
       " 'but',\n",
       " 'for',\n",
       " 'the',\n",
       " 'trained',\n",
       " 'reasoner',\n",
       " 'to',\n",
       " 'admit',\n",
       " 'such',\n",
       " 'intrusions',\n",
       " 'into',\n",
       " 'his',\n",
       " 'own',\n",
       " 'delicate',\n",
       " 'and',\n",
       " 'finely',\n",
       " 'adjusted',\n",
       " 'temperament',\n",
       " 'was',\n",
       " 'to',\n",
       " 'introduce',\n",
       " 'a',\n",
       " 'distracting',\n",
       " 'factor',\n",
       " 'which',\n",
       " 'might',\n",
       " 'throw',\n",
       " 'a',\n",
       " 'doubt',\n",
       " 'upon',\n",
       " 'all',\n",
       " 'his',\n",
       " 'mental',\n",
       " 'results',\n",
       " 'grit',\n",
       " 'in',\n",
       " 'a',\n",
       " 'sensitive',\n",
       " 'instrument',\n",
       " 'or',\n",
       " 'a',\n",
       " 'crack',\n",
       " 'in',\n",
       " 'one',\n",
       " 'of',\n",
       " 'his',\n",
       " 'own',\n",
       " 'high',\n",
       " 'power',\n",
       " 'lenses',\n",
       " 'would',\n",
       " 'not',\n",
       " 'be',\n",
       " 'more',\n",
       " 'disturbing',\n",
       " 'than',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'emotion',\n",
       " 'in',\n",
       " 'a',\n",
       " 'nature',\n",
       " 'such',\n",
       " 'as',\n",
       " 'his',\n",
       " 'and',\n",
       " 'yet',\n",
       " 'there',\n",
       " 'was',\n",
       " 'but',\n",
       " 'one',\n",
       " 'woman',\n",
       " 'to',\n",
       " 'him',\n",
       " 'and',\n",
       " 'that',\n",
       " 'woman',\n",
       " 'was',\n",
       " 'the',\n",
       " 'late',\n",
       " 'irene',\n",
       " 'adler',\n",
       " 'of',\n",
       " 'dubious',\n",
       " 'and',\n",
       " 'questionable',\n",
       " 'memory',\n",
       " 'i',\n",
       " 'had',\n",
       " 'seen',\n",
       " 'little',\n",
       " 'of',\n",
       " 'holmes',\n",
       " 'lately',\n",
       " 'my',\n",
       " 'marriage',\n",
       " 'had',\n",
       " 'drifted',\n",
       " 'us',\n",
       " 'away',\n",
       " 'from',\n",
       " 'each',\n",
       " 'other',\n",
       " 'my',\n",
       " 'own',\n",
       " 'complete',\n",
       " 'happiness',\n",
       " 'and',\n",
       " 'the',\n",
       " 'home',\n",
       " 'centred',\n",
       " 'interests',\n",
       " 'which',\n",
       " 'rise',\n",
       " 'up',\n",
       " 'around',\n",
       " 'the',\n",
       " 'man',\n",
       " 'who',\n",
       " 'first',\n",
       " 'finds',\n",
       " 'himself',\n",
       " 'master',\n",
       " 'of',\n",
       " 'his',\n",
       " 'own',\n",
       " 'establishment',\n",
       " 'were',\n",
       " 'sufficient',\n",
       " 'to',\n",
       " 'absorb',\n",
       " 'all',\n",
       " 'my',\n",
       " 'attention',\n",
       " 'while',\n",
       " 'holmes',\n",
       " 'who',\n",
       " 'loathed',\n",
       " 'every',\n",
       " 'form',\n",
       " 'of',\n",
       " 'society',\n",
       " 'with',\n",
       " 'his',\n",
       " 'whole',\n",
       " 'bohemian',\n",
       " 'soul',\n",
       " 'remained',\n",
       " 'in',\n",
       " 'our',\n",
       " 'lodgings',\n",
       " 'in',\n",
       " 'baker',\n",
       " 'street',\n",
       " 'buried',\n",
       " 'among',\n",
       " 'his',\n",
       " 'old',\n",
       " 'books',\n",
       " 'and',\n",
       " 'alternating',\n",
       " 'from',\n",
       " 'week',\n",
       " 'to',\n",
       " 'week',\n",
       " 'between',\n",
       " 'cocaine',\n",
       " 'and',\n",
       " 'ambition',\n",
       " 'the',\n",
       " 'drowsiness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'drug',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fierce',\n",
       " 'energy',\n",
       " 'of',\n",
       " 'his',\n",
       " 'own',\n",
       " 'keen',\n",
       " 'nature',\n",
       " 'he',\n",
       " 'was',\n",
       " 'still',\n",
       " 'as',\n",
       " 'ever',\n",
       " 'deeply',\n",
       " 'attracted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " 'crime',\n",
       " 'and',\n",
       " 'occupied',\n",
       " 'his',\n",
       " 'immense',\n",
       " 'faculties',\n",
       " 'and',\n",
       " 'extraordinary',\n",
       " 'powers',\n",
       " 'of',\n",
       " 'observation',\n",
       " 'in',\n",
       " 'following',\n",
       " 'out',\n",
       " 'those',\n",
       " 'clues',\n",
       " 'and',\n",
       " 'clearing',\n",
       " 'up',\n",
       " 'those',\n",
       " 'mysteries',\n",
       " 'which',\n",
       " 'had',\n",
       " 'been',\n",
       " 'abandoned',\n",
       " 'as',\n",
       " 'hopeless',\n",
       " 'by',\n",
       " 'the',\n",
       " 'official',\n",
       " 'police',\n",
       " 'from',\n",
       " 'time',\n",
       " 'to',\n",
       " 'time',\n",
       " 'i',\n",
       " 'heard',\n",
       " 'some',\n",
       " 'vague',\n",
       " 'account',\n",
       " 'of',\n",
       " 'his',\n",
       " 'doings',\n",
       " 'of',\n",
       " 'his',\n",
       " 'summons',\n",
       " 'to',\n",
       " 'odessa',\n",
       " 'in',\n",
       " 'the',\n",
       " 'case',\n",
       " 'of',\n",
       " 'the',\n",
       " 'trepoff',\n",
       " 'murder',\n",
       " 'of',\n",
       " 'his',\n",
       " 'clearing',\n",
       " 'up',\n",
       " 'of',\n",
       " 'the',\n",
       " 'singular',\n",
       " 'tragedy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'atkinson',\n",
       " 'brothers',\n",
       " 'at',\n",
       " 'trincomalee',\n",
       " 'and',\n",
       " 'finally',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mission',\n",
       " 'which',\n",
       " 'he',\n",
       " 'had',\n",
       " 'accomplished',\n",
       " 'so',\n",
       " 'delicately',\n",
       " 'and',\n",
       " 'successfully',\n",
       " 'for',\n",
       " 'the',\n",
       " 'reigning',\n",
       " 'family',\n",
       " 'of',\n",
       " 'holland',\n",
       " 'beyond',\n",
       " 'these',\n",
       " 'signs',\n",
       " 'of',\n",
       " 'his',\n",
       " 'activity',\n",
       " 'however',\n",
       " 'which',\n",
       " 'i',\n",
       " 'merely',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'all',\n",
       " 'the',\n",
       " 'readers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'daily',\n",
       " 'press',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'little',\n",
       " 'of',\n",
       " 'my',\n",
       " 'former',\n",
       " 'friend',\n",
       " 'and',\n",
       " 'companion',\n",
       " 'one',\n",
       " 'night',\n",
       " 'it',\n",
       " 'was',\n",
       " 'on',\n",
       " 'the',\n",
       " 'twentieth',\n",
       " 'of',\n",
       " 'march',\n",
       " '1888',\n",
       " 'i',\n",
       " 'was',\n",
       " 'returning',\n",
       " 'from',\n",
       " 'a',\n",
       " 'journey',\n",
       " 'to',\n",
       " 'a',\n",
       " 'patient',\n",
       " 'for',\n",
       " 'i',\n",
       " 'had',\n",
       " 'now',\n",
       " 'returned',\n",
       " 'to',\n",
       " 'civil',\n",
       " 'practice',\n",
       " 'when',\n",
       " 'my',\n",
       " 'way',\n",
       " 'led',\n",
       " 'me',\n",
       " 'through',\n",
       " 'baker',\n",
       " 'street',\n",
       " 'as',\n",
       " 'i',\n",
       " 'passed',\n",
       " 'the',\n",
       " 'well',\n",
       " 'remembered',\n",
       " 'door',\n",
       " 'which',\n",
       " 'must',\n",
       " 'always',\n",
       " 'be',\n",
       " 'associated',\n",
       " 'in',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'with',\n",
       " 'my',\n",
       " 'wooing',\n",
       " 'and',\n",
       " 'with',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'incidents',\n",
       " 'of',\n",
       " 'the',\n",
       " 'study',\n",
       " 'in',\n",
       " 'scarlet',\n",
       " 'i',\n",
       " 'was',\n",
       " 'seized',\n",
       " 'with',\n",
       " 'a',\n",
       " 'keen',\n",
       " 'desire',\n",
       " 'to',\n",
       " 'see',\n",
       " 'holmes',\n",
       " 'again',\n",
       " 'and',\n",
       " 'to',\n",
       " 'know',\n",
       " 'how',\n",
       " 'he',\n",
       " 'was',\n",
       " 'employing',\n",
       " 'his',\n",
       " 'extraordinary',\n",
       " 'powers',\n",
       " 'his',\n",
       " 'rooms',\n",
       " 'were',\n",
       " 'brilliantly',\n",
       " 'lit',\n",
       " 'and',\n",
       " 'even',\n",
       " 'as',\n",
       " 'i',\n",
       " 'looked',\n",
       " 'up',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'his',\n",
       " 'tall',\n",
       " 'spare',\n",
       " 'figure',\n",
       " 'pass',\n",
       " 'twice',\n",
       " 'in',\n",
       " 'a',\n",
       " 'dark',\n",
       " 'silhouette',\n",
       " 'against',\n",
       " 'the',\n",
       " 'blind',\n",
       " 'he',\n",
       " 'was',\n",
       " 'pacing',\n",
       " 'the',\n",
       " 'room',\n",
       " 'swiftly',\n",
       " 'eagerly',\n",
       " 'with',\n",
       " 'his',\n",
       " 'head',\n",
       " 'sunk',\n",
       " 'upon',\n",
       " 'his',\n",
       " 'chest',\n",
       " 'and',\n",
       " 'his',\n",
       " 'hands',\n",
       " 'clasped',\n",
       " 'behind',\n",
       " 'him',\n",
       " 'to',\n",
       " 'me',\n",
       " 'who',\n",
       " 'knew',\n",
       " 'his',\n",
       " 'every',\n",
       " 'mood',\n",
       " 'and',\n",
       " 'habit',\n",
       " 'his',\n",
       " 'attitude',\n",
       " 'and',\n",
       " 'manner',\n",
       " 'told',\n",
       " 'their',\n",
       " 'own',\n",
       " 'story',\n",
       " 'he',\n",
       " 'was',\n",
       " 'at',\n",
       " 'work',\n",
       " 'again',\n",
       " 'he',\n",
       " 'had',\n",
       " 'risen',\n",
       " 'out',\n",
       " 'of',\n",
       " 'his',\n",
       " 'drug',\n",
       " 'created',\n",
       " 'dreams',\n",
       " 'and',\n",
       " 'was',\n",
       " 'hot',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'scent',\n",
       " 'of',\n",
       " 'some',\n",
       " 'new',\n",
       " 'problem',\n",
       " 'i',\n",
       " 'rang',\n",
       " 'the',\n",
       " 'bell',\n",
       " 'and',\n",
       " 'was',\n",
       " 'shown',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'chamber',\n",
       " 'which',\n",
       " 'had',\n",
       " 'formerly',\n",
       " 'been',\n",
       " 'in',\n",
       " 'part',\n",
       " 'my',\n",
       " 'own',\n",
       " 'his',\n",
       " 'manner',\n",
       " 'was',\n",
       " 'not',\n",
       " 'effusive',\n",
       " 'it',\n",
       " 'seldom',\n",
       " 'was',\n",
       " 'but',\n",
       " 'he',\n",
       " 'was',\n",
       " 'glad',\n",
       " 'i',\n",
       " 'think',\n",
       " 'to',\n",
       " 'see',\n",
       " 'me',\n",
       " 'with',\n",
       " 'hardly',\n",
       " 'a',\n",
       " 'word',\n",
       " 'spoken',\n",
       " 'but',\n",
       " 'with',\n",
       " 'a',\n",
       " 'kindly',\n",
       " 'eye',\n",
       " 'he',\n",
       " 'waved',\n",
       " 'me',\n",
       " 'to',\n",
       " 'an',\n",
       " 'armchair',\n",
       " 'threw',\n",
       " 'across',\n",
       " 'his',\n",
       " 'case',\n",
       " 'of',\n",
       " 'cigars',\n",
       " 'and',\n",
       " 'indicated',\n",
       " 'a',\n",
       " 'spirit',\n",
       " 'case',\n",
       " 'and',\n",
       " 'a',\n",
       " 'gasogene',\n",
       " 'in',\n",
       " 'the',\n",
       " 'corner',\n",
       " 'then',\n",
       " 'he',\n",
       " 'stood',\n",
       " 'before',\n",
       " 'the',\n",
       " 'fire',\n",
       " 'and',\n",
       " 'looked',\n",
       " 'me',\n",
       " 'over',\n",
       " 'in',\n",
       " 'his',\n",
       " 'singular',\n",
       " 'introspective',\n",
       " 'fashion',\n",
       " 'wedlock',\n",
       " 'suits',\n",
       " 'you',\n",
       " 'he',\n",
       " 'remarked',\n",
       " 'i',\n",
       " 'think',\n",
       " 'watson',\n",
       " 'that',\n",
       " 'you',\n",
       " 'have',\n",
       " 'put',\n",
       " 'on',\n",
       " 'seven',\n",
       " 'and',\n",
       " 'a',\n",
       " 'half',\n",
       " 'pounds',\n",
       " 'since',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'you',\n",
       " 'seven',\n",
       " 'i',\n",
       " 'answered',\n",
       " 'indeed',\n",
       " 'i',\n",
       " 'should',\n",
       " 'have',\n",
       " 'thought',\n",
       " 'a',\n",
       " 'little',\n",
       " 'more',\n",
       " 'just',\n",
       " 'a',\n",
       " 'trifle',\n",
       " 'more',\n",
       " 'i',\n",
       " 'fancy',\n",
       " 'watson',\n",
       " 'and',\n",
       " 'in',\n",
       " 'practice',\n",
       " 'again',\n",
       " 'i',\n",
       " 'observe',\n",
       " 'you',\n",
       " 'did',\n",
       " 'not',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'that',\n",
       " 'you',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'go',\n",
       " 'into',\n",
       " 'harness',\n",
       " 'then',\n",
       " 'how',\n",
       " 'do',\n",
       " 'you',\n",
       " 'know',\n",
       " 'i',\n",
       " 'see',\n",
       " 'it',\n",
       " 'i',\n",
       " 'deduce',\n",
       " 'it',\n",
       " 'how',\n",
       " 'do',\n",
       " 'i',\n",
       " 'know',\n",
       " 'that',\n",
       " 'you',\n",
       " 'have',\n",
       " 'been',\n",
       " 'getting',\n",
       " 'yourself',\n",
       " 'very',\n",
       " 'wet',\n",
       " 'lately',\n",
       " 'and',\n",
       " 'that',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'most',\n",
       " 'clumsy',\n",
       " 'and',\n",
       " 'careless',\n",
       " 'servant',\n",
       " 'girl',\n",
       " 'my',\n",
       " 'dear',\n",
       " 'holmes',\n",
       " 'said',\n",
       " 'i',\n",
       " 'this',\n",
       " 'is',\n",
       " 'too',\n",
       " 'much',\n",
       " 'you',\n",
       " 'would',\n",
       " 'certainly',\n",
       " 'have',\n",
       " 'been',\n",
       " 'burned',\n",
       " 'had',\n",
       " 'you',\n",
       " 'lived',\n",
       " 'a',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#Split the document into tokens and remove any special charcaters if any.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e1b8a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 's', 'the', 'adventures']\n",
      "of\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Feature Engineering of words: Based on the problem statement we build features to train the ML model.\n",
    "#1. Form the unique words as dictionar\n",
    "unique_words=np.unique(words)\n",
    "unique_word_index=dict((c,i) for i,c in enumerate(unique_words)) #(word: index(0 to length of dataset))\n",
    "unique_word_index\n",
    "#2. Create 2 features next_word and prev_word , where prev_words will keep 5 previous words and next_words in the corresponding array.\n",
    "WORD_LENGTH=5\n",
    "prev_words=[]\n",
    "next_words=[]\n",
    "for i in range(len(words)-WORD_LENGTH):\n",
    "#     print(words)\n",
    "#     print(words[1:1+5]) #words[start_index:end_index-1]\n",
    "#     print(words[1+5])   #words[end_index]\n",
    "    prev_words.append(words[i:i + WORD_LENGTH])\n",
    "    next_words.append(words[i+WORD_LENGTH])\n",
    "print(prev_words[0]) #will have 5 words\n",
    "print(next_words[0]) #will have the 6th word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7910aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109221\n",
      "8201\n",
      "109221\n"
     ]
    }
   ],
   "source": [
    "print(len(prev_words)) #No of rows untill the end of document\n",
    "print(len(unique_words)) #No of unique words\n",
    "print(len(next_words))\n",
    "#X direction-> All the words \n",
    "#y direction-> Each unique word\n",
    "#Z direction-> Bag of words reprenstation for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87f7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words for each word where the word is present keep 1.\n",
    "X=np.zeros((len(prev_words),WORD_LENGTH,len(unique_words)),dtype=bool) #shape=(109221,5,8201) all with zeroes\n",
    "Y=np.zeros((len(next_words),len(unique_words)),dtype=bool) #shape=(109221,8201)\n",
    "\n",
    "#I will iterate x and y if the word is available so that the corresponding position becomes 1 in zth direction.\n",
    "for i ,each_word in enumerate(prev_words):\n",
    "    for j,each in enumerate(each_word):\n",
    "        X[i,j,unique_word_index[each]]=1\n",
    "    Y[i,unique_word_index[next_words[i]]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "384119de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False False False]\n",
      "(109221, 5, 8201)\n"
     ]
    }
   ],
   "source": [
    "print(X[0][0])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adeb2317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 128)               4264960   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8201)              1057929   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,322,889\n",
      "Trainable params: 5,322,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://www.youtube.com/watch?v=S0XFd0VMFss\n",
    "#https://www.youtube.com/watch?v=AsNTP8Kwu80\n",
    "#RNN\n",
    "#best one is LSTM for RNN\n",
    "#RNN->LSTM->Transformers\n",
    "#1. The model starts with Sequential and then LSTM can be middle layer . Dense layer is where the actual computation\n",
    "#of sigma and tahh happend . \n",
    "#2.Activation functions\n",
    "#1. relu [-5,0,9]-> [0,0,9] common one used with sense layer\n",
    "#2. sigmoid [-6,-7,0,9] -> [0.0008,0.0007,1.00,1.0098] ;value[<-5]~0;value[>5]~1 used of binary classification and multi-label\n",
    "#3. softmax[-8,-9,0,8]-> [0.2,0.5,0.1,0.2]=1 returns the probability of words and sums to 1.\n",
    "\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense,Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words)))) # 5,8201 \n",
    "model.add(Dense(len(unique_words),activation=\"softmax\")) #8201\n",
    "model.summary() \n",
    "#LSTM layer returns (x,number of units) . Dense layer returns (x,number of unique words)\n",
    "#Here the last layer was softmax because we want to predict one word from 8201 words with maximum probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "061ce693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dreddyag\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811/811 [==============================] - 78s 94ms/step - loss: 6.0109 - accuracy: 0.1064 - val_loss: 7.0681 - val_accuracy: 0.1078\n",
      "Epoch 2/2\n",
      "811/811 [==============================] - 88s 108ms/step - loss: 5.7739 - accuracy: 0.1475 - val_loss: 8.0000 - val_accuracy: 0.1084\n"
     ]
    }
   ],
   "source": [
    "#optimizer is gradient desecnt approach. Adam is most common one which has weighted sum of prev value.\n",
    "#RMSprop is simple gradient descent\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "#categorical_crossentropy Used as a loss function for multi-class classification model where there are two or more output labels\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=2, shuffle=True).history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8628ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.zeros((len(prev_words),WORD_LENGTH,len(unique_words)),dtype=bool) #shape=(109221,5,8201) all with zeroes\n",
    "Y=np.zeros((len(next_words),len(unique_words)),dtype=bool) #shape=(109221,8201)\n",
    "\n",
    "#I will iterate x and y if the word is available so that the corresponding position becomes 1 in zth direction.\n",
    "for i ,each_word in enumerate(prev_words):\n",
    "    for j,each in enumerate(each_word):\n",
    "        X[i,j,unique_word_index[each]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ea5cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_completions(text,word_length):\n",
    "    #1.Split the text\n",
    "    word=text.split()\n",
    "    #2.create zero matrix (1 sentence, 5 length, vocabulary length)\n",
    "    test_data=np.zeros((1,word_length,len(unique_words)),dtype=bool)\n",
    "    #3.keep 1 where ever word is present\n",
    "    for i,w in enumerate(word):\n",
    "        test_data[0,i,unique_word_index[w]]=1\n",
    "    #4.Predict the test data\n",
    "    y_hat=model.predict(test_data)\n",
    "    #5.it returns probabilities of words and the one with maximum probability is the word\n",
    "    y_hat_index=np.argmax(y_hat)\n",
    "    #6.Lookup the word is maximum prob in vocabulary\n",
    "    return unique_words[y_hat_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4e4ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Someting: I love project gutenberg\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "and\n",
      "To Exit Press : (Y)N\n",
      "Enter Someting: project gutenberg s the project \n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "of\n",
      "To Exit Press : (Y)n\n",
      "Enter Someting: project gutenberg s the ebook\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "of\n",
      "To Exit Press : (Y)Y\n"
     ]
    }
   ],
   "source": [
    "boolx=\"N\"\n",
    "while boolx!=\"Y\":\n",
    "    try:\n",
    "        seq=input(\"Enter Someting: \").lower()\n",
    "    except:\n",
    "        seq=\"It is not a lack of love, but a lack of\"\n",
    "    print(predict_completions(seq, 5))\n",
    "    boolx=input(\"To Exit Press : (Y)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5909fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
